#!/bin/bash
set -euo pipefail

# ðŸŽ¨ Colores
YELLOW='\033[1;33m'
GREEN='\033[1;32m'
RED='\033[1;31m'
BLUE='\033[1;34m'
NC='\033[0m'

# ðŸ§  Variables
DEPTH=4
KATANA_EXTRA=""
GOSPIDER_EXTRA=""
VERBOSE=0
OUTDIR=""
DOMAIN=""
LIST_FILE=""

# ðŸ“œ Ayuda
function usage() {
cat <<EOF
Uso: $0 [opciones] dominio.com

Opciones:
  -h, --help            Mostrar esta ayuda
  -t archivo.txt        Usar una lista de dominios
  -o carpeta            Carpeta de salida personalizada
  -v                    Verbose nivel 1
  -vv                   Verbose nivel 2
  --deep                Crawling profundo (mÃ¡s JS)
EOF
exit 1
}

# ðŸ§© Verbosidad
log()   { [[ $VERBOSE -ge 1 ]] && echo -e "${YELLOW}[v] $*${NC}"; }
logv()  { [[ $VERBOSE -ge 2 ]] && echo -e "${BLUE}[vv] $*${NC}"; }

# âœ… Dependencias
function check_dependencies() {
    echo "[*] Verificando herramientas necesarias..."
    for cmd in katana gospider uro python3; do
        if ! command -v "$cmd" >/dev/null 2>&1; then
            echo -e "${RED}[-] Falta la herramienta: $cmd${NC}"
            exit 1
        fi
    done
}

# ðŸ” Katana
function run_katana() {
    echo "[*] Katana..."
    katana -u "$1" -jc -iqp -rl 150 -v $KATANA_EXTRA | grep -o 'http[^ ]*' | uro > "$2/katana.txt"
    logv "Katana finalizado."
}

# ðŸ” Gospider
function run_gospider() {
    echo "[*] Gospider..."
    gospider -s "$1" -q --sitemap --robots --no-redirect -v --subs -d "$DEPTH" --user-agent web $GOSPIDER_EXTRA | uro > "$2/gospider.txt"
    logv "Gospider finalizado."
}

# ðŸ•¸ WebSpider
function run_webspider() {
    echo "[*] WebSpider.py..."
    [[ -f data.txt ]] && rm -f data.txt
    python3 "$(dirname "$0")/WebSpider.py" "$1"
    mv data.txt "$2/webspider.txt"
    logv "WebSpider finalizado."
}

# ðŸ§¹ UnificaciÃ³n y anÃ¡lisis
function unify_results() {
    echo "[*] Unificando resultados..."
    cat "$1"/*.txt | sort -u > "$1/crawler_combined.txt"

    grep -Ei '\.js$|\.php$|\.json$|/api/|admin|login|logout|debug' "$1/crawler_combined.txt" > "$1/crawler_interesting.txt"

    grep -Ei 'apikey|token|auth|pass|secret|key=' "$1/crawler_combined.txt" > "$1/posibles_secretos.txt" || true

    cp "$1/crawler_combined.txt" "$1/urls.txt"
    logv "UnificaciÃ³n finalizada."
}

# ðŸ“Š Resumen
function print_summary() {
    echo -e "\n${GREEN}[âœ“] Crawler finalizado.${NC}"
    echo "[*] Unificadas:     $1/crawler_combined.txt ($(wc -l < "$1/crawler_combined.txt") URLs)"
    echo "[*] Interesantes:   $1/crawler_interesting.txt ($(wc -l < "$1/crawler_interesting.txt") coincidencias)"
    echo "[*] Secretos:       $1/posibles_secretos.txt ($(wc -l < "$1/posibles_secretos.txt" 2>/dev/null || echo 0))"
    echo "[*] Para anÃ¡lisis:  $1/urls.txt"
}

# ðŸš¨ Argumentos
while [[ $# -gt 0 ]]; do
    case "$1" in
        -h|--help) usage ;;
        -v) VERBOSE=1 ;;
        -vv) VERBOSE=2 ;;
        -t) LIST_FILE="$2"; shift ;;
        -o) OUTDIR="$2"; shift ;;
        --deep)
            DEPTH=6
            KATANA_EXTRA="-d 3"
            GOSPIDER_EXTRA="--js"
            ;;
        -*)
            echo "OpciÃ³n desconocida: $1"
            usage
            ;;
        *)
            DOMAIN="$1"
            ;;
    esac
    shift
done

# ðŸš¦ ValidaciÃ³n
if [[ -z "$DOMAIN" && -z "$LIST_FILE" ]]; then
    echo -e "${RED}[!] Debes especificar un dominio o lista con -t${NC}"
    usage
fi

check_dependencies

# ðŸš€ Modo lista
if [[ -n "$LIST_FILE" ]]; then
    while read -r DOMAIN; do
        [[ -z "$DOMAIN" ]] && continue
        TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
        DIR="${OUTDIR:-crawler_output}/${DOMAIN}_${TIMESTAMP}"
        mkdir -p "$DIR"
        echo -e "\n[+] Objetivo: $DOMAIN"
        run_katana "$DOMAIN" "$DIR" &
        run_gospider "$DOMAIN" "$DIR" &
        run_webspider "$DOMAIN" "$DIR" &
        wait
        unify_results "$DIR"
        print_summary "$DIR"
    done < "$LIST_FILE"
else
    TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
    DIR="${OUTDIR:-crawler_output}/${DOMAIN}_${TIMESTAMP}"
    mkdir -p "$DIR"
    echo "[+] Objetivo: $DOMAIN"
    run_katana "$DOMAIN" "$DIR" &
    run_gospider "$DOMAIN" "$DIR" &
    run_webspider "$DOMAIN" "$DIR" &
    wait
    unify_results "$DIR"
    print_summary "$DIR"
fi
